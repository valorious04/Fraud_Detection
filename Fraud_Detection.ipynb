# 1. Install dependencies (only once, in terminal / command prompt)

# pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn xgboost

# 2. Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

from imblearn.over_sampling import SMOTE

# 3. Load dataset from your computer

fraud_data_path = r"E:\DS internshala\Fraud.csv"
data_dict_path  = r"E:\DS internshala\Data Dictionary.csv"

data = pd.read_csv(fraud_data_path)
print("Dataset Shape:", data.shape)
print(data.head())

# If you also want to use data dictionary
try:
    data_dict = pd.read_csv(data_dict_path)
    print("\nData Dictionary Loaded Successfully")
except:
    print("\n⚠️ Data Dictionary not found or not needed.")

# 4. Data Preprocessing
# Drop unnecessary ID columns
if "nameOrig" in data.columns:
    data = data.drop("nameOrig", axis=1)
if "nameDest" in data.columns:
    data = data.drop("nameDest", axis=1)

# Encode categorical variable 'type'
if "type" in data.columns:
    data = pd.get_dummies(data, columns=["type"], drop_first=True)

# Handle missing values
num_cols = data.select_dtypes(include=[np.number]).columns
for col in num_cols:
    data[col] = data[col].fillna(data[col].median())

# Features and target
X = data.drop(["isFraud", "isFlaggedFraud"], axis=1, errors='ignore')
y = data["isFraud"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Handle imbalance with SMOTE
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
print("Before SMOTE:", y_train.value_counts())
print("After SMOTE:", y_train_res.value_counts())

# Scale numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

# 5. Model Training
# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train_res)
y_pred_lr = log_reg.predict(X_test_scaled)

print("\nLogistic Regression Results:")
print(classification_report(y_test, y_pred_lr))

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_res, y_train_res)
y_pred_rf = rf.predict(X_test)

print("\nRandom Forest Results:")
print(classification_report(y_test, y_pred_rf))

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100, learning_rate=0.1, max_depth=6,
    random_state=42, use_label_encoder=False, eval_metric="logloss"
)
xgb_model.fit(X_train_res, y_train_res)
y_pred_xgb = xgb_model.predict(X_test)

print("\nXGBoost Results:")
print(classification_report(y_test, y_pred_xgb))
